{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "# %matplotlib inline\n",
    "import argparse\n",
    "import os\n",
    "import os.path as osp\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torch import Tensor\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "import utils\n",
    "from utils.dataset import SegmentationDataSet\n",
    "from utils.printer import source_printer\n",
    "from utils.printer import target_printer\n",
    "import utils.model as model\n",
    "from utils.model import UNet\n",
    "from utils.model import YNet\n",
    "from utils.model import Recons_net\n",
    "from utils.model import ClassifNet\n",
    "from utils.utils import getLossAccuracyOnDataset\n",
    "from utils.utils import preprocessing\n",
    "from utils.utils import IoU\n",
    "from utils.utils import postprocessing\n",
    "from utils.utils import dice_coeff\n",
    "from utils.utils import multiclass_dice_coeff\n",
    "from utils.utils import dice_loss\n",
    "from utils.utils import smooth\n",
    "\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "manualSeed = 999\n",
    "# manualSeed = random.randint(1, 10000) # use if you want new results\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_show(img, cmap=\"gray\", title=\"\"):\n",
    "    # cv.namedWindow(title, cv.WINDOW_NORMAL)\n",
    "    # cv.imshow(title, img)\n",
    "    print(title)\n",
    "    plt.imshow(img, cmap)\n",
    "    # display that image\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def tensor_to_image(tensor):\n",
    "    transform = transforms.ToPILImage()\n",
    "    return transform(tensor)\n",
    "\n",
    "\n",
    "def image_to_tensor(image):\n",
    "    transform_1 = transforms.ToPILImage()\n",
    "    transform_2 = transforms.ToTensor()\n",
    "    img_tensor = transform_2(transform_1(image))\n",
    "    return img_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = str(datetime.now()).split(' ')[0]\n",
    "heure = str(datetime.now()).split(' ')[1].split('.')[0]\n",
    "\n",
    "# Root directory for dataset\n",
    "# Refaire nos data folder et tout pour que ce soit\n",
    "# au format demandé par le dataloader\n",
    "\n",
    "training_set_name = \"MNIST_STUDENT\"+\"_\"+str(date)+\"_\"+str(heure)\n",
    "\n",
    "\n",
    "dataset_folder = \"xxx\"\n",
    "train_list = dataset_folder+\"patches/xxx.txt\"\n",
    "test_list = dataset_folder+\"patches/xxx.txt\"\n",
    "\n",
    "\n",
    "# Number of workers for dataloader\n",
    "workers = 10\n",
    "\n",
    "# Batch size during training (low batch_size if there are memory issues)\n",
    "batch_size = 10\n",
    "\n",
    "# Spatial size of training images. All images will be resized to this\n",
    "#   size using a transformer.\n",
    "image_size_source = 256\n",
    "image_size_target = 256\n",
    "image_size_discriminator = 64\n",
    "\n",
    "# Number of channels in the training images. For color images this is 3\n",
    "nc = 3\n",
    "\n",
    "# Number of training epochs\n",
    "num_epochs = 20\n",
    "\n",
    "# Learning rate for optimizers\n",
    "learning_rate = 1e-5  # e-5\n",
    "\n",
    "# Beta1 hyperparam for Adam optimizers\n",
    "beta1 = 0.5\n",
    "\n",
    "# Number of GPUs available. Use 0 for CPU mode.\n",
    "ngpu = 1\n",
    "\n",
    "# Saves every batch_save_interval\n",
    "batch_save_interval = 2\n",
    "\n",
    "# some net variable\n",
    "amp = False\n",
    "\n",
    "save_folder =  \"../Data/Saves\" + training_set_name\n",
    "\n",
    "if not os.path.exists(\"../Data\"):\n",
    "    os.mkdir(\"../Data\")\n",
    "if not os.path.exists(\"../Data/Saves\" ):\n",
    "    os.mkdir(\"../Data/Saves\" )\n",
    "\n",
    "# We create this folder (only if it doesn't exists) to save weights of the training at some keys epoch\n",
    "if not os.path.exists(save_folder):\n",
    "    os.mkdir(save_folder)\n",
    "    os.mkdir(save_folder+\"/loss-dice_listes\")\n",
    "    os.mkdir(save_folder+\"/newtork_weigths\")\n",
    "    os.mkdir(save_folder+\"/training_monitoring\")\n",
    "\n",
    "log_file = open(save_folder+\"/log+\"+\"_\"+date+\"_\"+heure+\".txt\", \"w\")\n",
    "\n",
    "log_file.write(\"dataset_folder :\"+dataset_folder+\"\\n\")\n",
    "log_file.write(\"batch_size=\"+str(batch_size)+\"\\n\")\n",
    "log_file.write(\"learning_rate_net=\"+str(learning_rate)+\"\\n\")\n",
    "log_file.write(\"num_epoch=\"+str(num_epochs)+\"\\n\")\n",
    "log_file.write(\"nc=\"+str(nc)+\"\\n\")\n",
    "log_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"number of gpus :\", torch.cuda.device_count())\n",
    "\n",
    "# Decide which device we want to run on\n",
    "device = torch.device(\"cuda:0\" if (\n",
    "    torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "\n",
    "# On regarde l'identifiant du GPU ou CPU sur lequel on travaille\n",
    "print(\"device ID\", device)\n",
    "print(\"nom du GPU\", torch.cuda.get_device_name(device))  # On vérifie son \"nom\"\n",
    "\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For data augmentation\n",
    "\n",
    "geometric_augs = [\n",
    "    # transforms.Resize((256, 256)), # Makes it easier to process using net\n",
    "    # transforms.RandomRotation(degrees=(0, 180)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    # transforms.RandomRotation(45),\n",
    "]\n",
    "\n",
    "color_augs = [\n",
    "    # transforms.ColorJitter(hue=0.05, saturation=0.4)\n",
    "]\n",
    "\n",
    "\n",
    "def make_tfs(augs):\n",
    "    return transforms.Compose([transforms.ToPILImage()]+augs + [transforms.ToTensor()])\n",
    "\n",
    "\n",
    "tfs = transforms.Compose(geometric_augs)\n",
    "\n",
    "\"\"\"\n",
    "# Importation des images et masques de i3\n",
    "dataset = SegmentationDataSet(root=dataset_folder,\n",
    "                              list_path=train_list,\n",
    "                              transform_img=make_tfs(\n",
    "                                  geometric_augs + color_augs),\n",
    "                              transform_label=make_tfs(geometric_augs)\n",
    "                              )\n",
    "\"\"\"\n",
    "\n",
    "torch.set_grad_enabled(True)\n",
    "\n",
    "data = dset.MNIST(\"/var/tmp/\", train=True, download=True,\n",
    "                  transform=transforms.Compose([\n",
    "                      transforms.ToTensor(),\n",
    "                      transforms.Normalize(\n",
    "                          (0.1307,), (0.3081,))\n",
    "                  ]))\n",
    "\n",
    "n_entries = len(data)\n",
    "train_split = 70/100\n",
    "validation_split = 50/100\n",
    "\n",
    "n_train = int(train_split*n_entries)\n",
    "n_test = int(((1-train_split)*(1-validation_split))*n_entries)\n",
    "n_validation = int(((1-train_split)*(validation_split))*n_entries)\n",
    "\n",
    "if (n_train + n_test + n_validation) != n_entries:\n",
    "    # Add one to the validation set\n",
    "    n_validation+=1\n",
    "\n",
    "print(\"Number of entries\", n_entries)\n",
    "print(\"Number of training entries\", n_train)\n",
    "print(\"Number of testing entries\", n_test)\n",
    "print(\"Number of validation entries\", n_validation)\n",
    "print(\"SUM =\", n_train+n_test+n_validation)\n",
    "\n",
    "train_data, test_data, validation_data = random_split(\n",
    "    data, [n_train, n_test, n_validation])\n",
    "\n",
    "\n",
    "filtered_7_indices = []\n",
    "filtered_7_5_indices = []\n",
    "filtered_7_5_9_indices = []\n",
    "filtered_7_5_9_2_indices = []\n",
    "for i in range(len(train_data)):\n",
    "    data_class = train_data[i][1]\n",
    "    if data_class != 7:\n",
    "        filtered_7_indices.append(i)\n",
    "        if data_class != 5:\n",
    "            filtered_7_5_indices.append(i)\n",
    "            if data_class != 9:\n",
    "                filtered_7_5_9_indices.append(i)\n",
    "                if data_class != 2:\n",
    "                    filtered_7_5_9_2_indices.append(i)\n",
    "\n",
    "    if i < 10:\n",
    "        print(i, train_data[i][1])\n",
    "\n",
    "\n",
    "filtered_7_train_data = torch.utils.data.Subset(train_data, filtered_7_indices)\n",
    "filtered_7_5_train_data = torch.utils.data.Subset(train_data, filtered_7_5_indices)\n",
    "filtered_7_5_9_train_data = torch.utils.data.Subset(train_data, filtered_7_5_9_indices)\n",
    "filtered_7_5_9_2_train_data = torch.utils.data.Subset(\n",
    "    train_data, filtered_7_5_9_2_indices)\n",
    "\n",
    "\n",
    "print(\"Size after filtering:\")\n",
    "print(len(filtered_7_train_data))\n",
    "print(len(filtered_7_5_train_data))\n",
    "print(len(filtered_7_5_9_train_data))\n",
    "print(len(filtered_7_5_9_2_train_data))\n",
    "\n",
    "\n",
    "# Dataloader\n",
    "filtered_7_train_dataloader = torch.utils.data.DataLoader(filtered_7_train_data,\n",
    "                                               batch_size=batch_size,\n",
    "                                               shuffle=True,\n",
    "                                               num_workers=workers)\n",
    "\n",
    "filtered_7_5_train_dataloader = torch.utils.data.DataLoader(filtered_7_5_train_data,\n",
    "                                                        batch_size=batch_size,\n",
    "                                                        shuffle=True,\n",
    "                                                        num_workers=workers)\n",
    "\n",
    "filtered_7_5_9_train_dataloader = torch.utils.data.DataLoader(filtered_7_5_9_train_data,\n",
    "                                                        batch_size=batch_size,\n",
    "                                                        shuffle=True,\n",
    "                                                        num_workers=workers)\n",
    "\n",
    "filtered_7_5_9_2_train_dataloader = torch.utils.data.DataLoader(filtered_7_5_9_train_data,\n",
    "                                                              batch_size=batch_size,\n",
    "                                                              shuffle=True,\n",
    "                                                              num_workers=workers)\n",
    "\n",
    "\n",
    "print(\"Sample of classes contained in filtered 7 :\")\n",
    "print(next(iter(filtered_7_train_dataloader))[1])\n",
    "print(\"Sample of classes contained in filtered 7 5 9 2: \")\n",
    "print(next(iter(filtered_7_5_9_2_train_dataloader))[1])\n",
    "\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_data,\n",
    "                                               batch_size=batch_size,\n",
    "                                               shuffle=True,\n",
    "                                               num_workers=workers)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data,\n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=True,\n",
    "                                              num_workers=workers)\n",
    "\n",
    "validation_dataloader = torch.utils.data.DataLoader(validation_data,\n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=True,\n",
    "                                              num_workers=workers)\n",
    "\n",
    "\n",
    "batch = next(iter(train_dataloader))\n",
    "\n",
    "# On affiche quelques exemple du batch pour vérifier qu'on a bien importé les données\n",
    "print(\"images source : \", batch[0].shape)\n",
    "print(\"mask source :\", batch[1].shape)\n",
    "\n",
    "\n",
    "print(batch[1])\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Training images source - a batch\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(batch[0].to(\n",
    "    device)[:64], padding=2, normalize=True).cpu(), (1, 2, 0)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural net architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load or create neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_generic_model(model, path=\"\"):\n",
    "    if len(path)>0:\n",
    "        state_dict = torch.load(path)\n",
    "        model.load_state_dict(state_dict)\n",
    "    model.to(device=device)\n",
    "\n",
    "    if (device.type == 'cuda') and (ngpu > 1):\n",
    "        print(\"Data Parallel\")\n",
    "        model = nn.DataParallel(model, list(range(ngpu)))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def load_student_model():\n",
    "    # STUDENT\n",
    "    return load_generic_model(model.StudentNetworkSmall())\n",
    "\n",
    "\n",
    "def load_teacher_model():\n",
    "    # TEACHER\n",
    "    return load_generic_model(model.TeacherNetwork())\n",
    "\n",
    "# Other\n",
    "\n",
    "def save_model(model, path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "\n",
    "# On revérifie qu'on tourne bien le réseau de neuronnes sur le GPU\n",
    "print(\"We are running U-Net on :\", torch.cuda.get_device_name(device))\n",
    "\n",
    "print(\"Teacher network summary\")\n",
    "summary(load_teacher_model(), (1, 28, 28))\n",
    "\n",
    "print(\"Student network summary\")\n",
    "summary(load_student_model(), (1, 28, 28))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainStep(model, data, optimizer):\n",
    "\t\"\"\"\n",
    "\tOne training step of the network: forward prop + backprop + update parameters\n",
    "\tReturn: (loss, accuracy) of current batch\n",
    "\t\"\"\"\n",
    "\tX, y = data\n",
    "\n",
    "\tX = X.to(\n",
    "            device=device, dtype=torch.float32)\n",
    "\ty = y.to(\n",
    "\t\tdevice=device, dtype=torch.long)\n",
    "\t\n",
    "\toptimizer.zero_grad()\n",
    "\tpred = model(X)\n",
    "\tloss = F.cross_entropy(pred, y)\n",
    "\tloss.backward()\n",
    "\toptimizer.step()\n",
    "\taccuracy = float(torch.sum(torch.argmax(\n",
    "\t\tpred, dim=1) == y).item()) / y.shape[0]\n",
    "\treturn loss, accuracy\n",
    "\n",
    "\n",
    "def validateStep(model, data):\n",
    "\tX, y = data\n",
    "\n",
    "\tX = X.to(\n",
    "            device=device, dtype=torch.float32)\n",
    "\ty = y.to(\n",
    "\t\tdevice=device, dtype=torch.long)\n",
    "\n",
    "\tpred = model(X)\n",
    "\tloss = F.cross_entropy(pred, y).item()\n",
    "\taccuracy = float(torch.sum(torch.argmax(\n",
    "\t\tpred, dim=1) == y).item()) / y.shape[0]\n",
    "\treturn loss, accuracy\n",
    "\n",
    "\n",
    "teacher_model = load_teacher_model()\n",
    "\n",
    "\n",
    "def studentTrainStepDistillationMNIST(model, data, optimizer):\n",
    "\t\"\"\"\n",
    "\tOne training step of student network: forward prop + backprop + update parameters\n",
    "\tReturn: (loss, accuracy) of current batch\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tX, y = data\n",
    "\n",
    "\tX = X.to(\n",
    "            device=device, dtype=torch.float32)\n",
    "\ty = y.to(\n",
    "\t\tdevice=device, dtype=torch.long)\n",
    "\t\n",
    "\tT = 1.0  # temperature for distillation loss\n",
    "\t# Using a higher value for T produces a softer probability distribution over classes\n",
    "\talpha = 1.0\n",
    "\t# trade-off between soft-target (st) cross-entropy and true-target (tt) cross-entropy;\n",
    "\t# loss = alpha * st + (1 - alpha) * tt\n",
    "\toptimizer.zero_grad()\n",
    "\tteacher_pred = None\n",
    "\tif (alpha > 0):\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tteacher_model.eval()\n",
    "\t\t\tteacher_pred = teacher_model(X)\n",
    "\tstudent_pred = model(X)\n",
    "\tloss = studentLoss(teacher_pred, student_pred, y, T, alpha)\n",
    "\tloss.backward() # Generates error : element 0 of tensors does not require grad and does not have a grad_fn\n",
    "\toptimizer.step()\n",
    "\taccuracy = float(torch.sum(torch.argmax(\n",
    "\t\tstudent_pred, dim=1) == y).item()) / y.shape[0]\n",
    "\treturn loss, accuracy\n",
    "\n",
    "\n",
    "\n",
    "def studentTrainStep(teacher_net, student_net, studentLossFunction, optimizer, X, y, T, alpha):\n",
    "\t\"\"\"\n",
    "\tOne training step of student network: forward prop + backprop + update parameters\n",
    "\tReturn: (loss, accuracy) of current batch\n",
    "\t\"\"\"\n",
    "\toptimizer.zero_grad()\n",
    "\tteacher_pred = None\n",
    "\tif (alpha > 0):\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tteacher_pred = teacher_net(X)\n",
    "\tstudent_pred = student_net(X)\n",
    "\tloss = studentLossFunction(teacher_pred, student_pred, y, T, alpha)\n",
    "\tloss.backward()\n",
    "\toptimizer.step()\n",
    "\taccuracy = float(torch.sum(torch.argmax(\n",
    "\t\tstudent_pred, dim=1) == y).item()) / y.shape[0]\n",
    "\treturn loss, accuracy\n",
    "\n",
    "def studentLoss(teacher_pred, student_pred, y, T, alpha):\n",
    "\t\"\"\"\n",
    "\t\tLoss function for student network: Loss = alpha * (distillation loss with soft-target) + (1 - alpha) * (cross-entropy loss with true label)\n",
    "\t\tReturn: loss\n",
    "\t\t\"\"\"\n",
    "\tif (alpha > 0):\n",
    "\t\tloss = F.kl_div(F.log_softmax(student_pred / T, dim=1), F.softmax(teacher_pred / T, dim=1),\n",
    "\t\t\t\t\t\treduction='batchmean') * (T ** 2) * alpha + F.cross_entropy(student_pred, y) * (1 - alpha)\n",
    "\telse:\n",
    "\t\tloss = F.cross_entropy(student_pred, y)\n",
    "\treturn loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Function \n",
    "def train(model, train_loader, validate_loader, trainStepFun, validateStepFun, epochs=20):\n",
    "    \n",
    "    scheduler_params = dict(max_lr=1e-3,\n",
    "                            epochs=epochs, steps_per_epoch=len(train_loader))\n",
    "\n",
    "    optimizer = optim.AdamW(list(model.parameters()))\n",
    "    scheduler_global = optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, **scheduler_params)  # goal: maximize Dice score\n",
    "    grad_scaler_global = torch.cuda.amp.GradScaler(\n",
    "        enabled=amp)  # Default parameter\n",
    "    \n",
    "    best_accuracy = 0.0\n",
    "    interval = []\n",
    "    interval_count = 0\n",
    "    validation_accuracy_progression = []\n",
    "    validation_loss_progression = []\n",
    "    print(\"Begin training...\") \n",
    "    for epoch in range(1, epochs+1):\n",
    "        running_train_loss = 0.0\n",
    "        running_accuracy = 0.0\n",
    "        running_vall_loss = 0.0\n",
    "        total = 0\n",
    "\n",
    "        with tqdm(total=n_train, desc=f'Epoch {epoch}/{epochs}', unit='img') as pbar_train:\n",
    "            # Training Loop\n",
    "            it = 0\n",
    "            epoch_loss = 0\n",
    "            epoch_acc = 0\n",
    "            for data in train_loader: \n",
    "                model.train()\n",
    "                loss, acc = trainStepFun(model, data, optimizer)\n",
    "                running_train_loss += loss #  # track the loss value\n",
    "                \n",
    "                it+=data[0].shape[0]\n",
    "                epoch_loss += loss\n",
    "                epoch_acc += acc\n",
    "\n",
    "                pbar_train.update(data[0].shape[0])\n",
    "                pbar_train.set_postfix(**{'loss (batch)': epoch_loss/it, \"accuracy (batch)\": epoch_acc/it})\n",
    "    \n",
    "            # Calculate training loss value \n",
    "            train_loss_value = running_train_loss/len(train_loader)\n",
    "\n",
    "        \n",
    "        with tqdm(total=n_train, desc=f'Epoch {epoch}/{epochs}', unit='img') as pbar_validate:\n",
    "            # Validation Loop \n",
    "            with torch.no_grad(): # Why not ?\n",
    "                for data in validate_loader:\n",
    "                    model.eval()\n",
    "                    ## loss, acc = trainStepFun(model, data)\n",
    "                    loss, acc = validateStepFun(model, data)\n",
    "\n",
    "                    # The label with the highest value will be our prediction \n",
    "                    running_vall_loss += loss\n",
    "                    total += 1\n",
    "                    running_accuracy += acc\n",
    "                                                \n",
    "                    validation_accuracy_progression.append(acc)\n",
    "                    validation_loss_progression.append(loss)\n",
    "                    interval_count += 1\n",
    "                    interval.append(interval_count)\n",
    "    \n",
    "        # Calculate validation loss value \n",
    "        val_loss_value = running_vall_loss/len(validate_loader) \n",
    "                \n",
    "        # Calculate accuracy as the number of correct predictions in the validation batch divided by the total number of predictions done.  \n",
    "        accuracy = (100.0 * running_accuracy / total)     \n",
    " \n",
    "        # Save the model if the accuracy is the best \n",
    "        if accuracy > best_accuracy:\n",
    "            torch.save(model.state_dict(), save_folder +\n",
    "                       \"/newtork_weigths/model_epoch{:}_validation_accuracy{:.3f}_train_loss{:.3f}.pth\".format(epoch, accuracy, train_loss_value))\n",
    "            best_accuracy = accuracy\n",
    "         \n",
    "        # Print the statistics of the epoch \n",
    "        print('Completed training batch', epoch, 'Training Loss is: %.4f' %train_loss_value,\n",
    "               'Validation Loss is: %.4f' %val_loss_value, 'Accuracy is %d %%' % (accuracy))\n",
    "        \"\"\"\n",
    "        validation_accuracy_smooth = smooth(validation_accuracy_progression, 0.99)\n",
    "        validation_loss_smooth = smooth(validation_loss_progression, 0.99)\n",
    "\n",
    "        plt.figure(1)\n",
    "        plt.clf()\n",
    "        plt.plot(interval, validation_accuracy_smooth,\n",
    "                 'g-', label='Global accuracy')\n",
    "        plt.xlabel(\"Iterations\")\n",
    "        plt.ylabel(\"Accuracy Value\")\n",
    "        plt.title(\"Accuracy Monitoring among validation\")\n",
    "        plt.legend()\n",
    "        plt.savefig(save_folder+\"/training_monitoring/accuracy_\" +\n",
    "                    str(epoch)+\"_epoch.png\")\n",
    "        \n",
    "        if (epoch == epochs):\n",
    "            plt.show()\n",
    "\n",
    "        plt.figure(1)\n",
    "        plt.clf()\n",
    "        plt.plot(interval, validation_loss_smooth,\n",
    "                 'r-', label='Global loss')\n",
    "        plt.xlabel(\"Iterations\")\n",
    "        plt.ylabel(\"Loss Value\")\n",
    "        plt.title(\"Loss Monitoring among validation\")\n",
    "        plt.legend()\n",
    "        plt.savefig(save_folder+\"/training_monitoring/loss_\" +\n",
    "                    str(epoch)+\"_epoch.png\")\n",
    "        \n",
    "        if (epoch == epochs):\n",
    "            plt.show()\n",
    "        \"\"\"\n",
    "\n",
    "    # Return the fully trained model\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to test the model\n",
    "def test(model, test_loader):\n",
    "    running_accuracy = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            inputs, outputs = data\n",
    "            outputs = outputs.to(torch.float32)\n",
    "            predicted_outputs = model(inputs)\n",
    "            _, predicted = torch.max(predicted_outputs, 1)\n",
    "            total += outputs.size(0)\n",
    "            running_accuracy += (predicted == outputs).sum().item()\n",
    "\n",
    "        print('Accuracy of the model based on the test set',\n",
    "              'inputs is: %d %%' % (100 * running_accuracy / total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed training batch 2 Training Loss is: 0.3887 Validation Loss is: 0.3491 Accuracy is 90 %\n",
      "Completed training batch 3 Training Loss is: 0.3264 Validation Loss is: 0.3146 Accuracy is 90 %\n"
     ]
    }
   ],
   "source": [
    "parameters = []\n",
    "models = []\n",
    "\n",
    "\n",
    "# Train models without distillation\n",
    "for i in range(0, 10):\n",
    "    for j in range(0, 10):\n",
    "      l1_size = int(30+i*5)\n",
    "      l2_size = int(10+j*5)\n",
    "      parameters.append((i, j))\n",
    "      model = train(load_generic_model(model.GenericNetwork(l1_size, l2_size)), train_dataloader, validation_dataloader, trainStep, validateStep)\n",
    "      models.append(model)\n",
    "      save_model(model, save_folder+\"/newtork_weigths/no_distillation_first_layer_{:}_second_layer_{:f}.pth\")\n",
    "\n",
    "# Train models with distillation\n",
    "\n",
    "\"\"\"\n",
    "train(load_student_model(), train_dataloader, validation_dataloader,\n",
    "      trainStep, validateStep)\n",
    "\n",
    "print(\"7 Filtering\")\n",
    "train(load_student_model(), filtered_7_train_dataloader, validation_dataloader,\n",
    "      trainStep, validateStep)\n",
    "\n",
    "print(\"7 5 Filtering\")\n",
    "train(load_student_model(), filtered_7_5_train_dataloader, validation_dataloader,\n",
    "      trainStep, validateStep)\n",
    "\n",
    "print(\"7 5 9 Filtering\")\n",
    "train(load_student_model(), filtered_7_5_9_train_dataloader, validation_dataloader,\n",
    "      trainStep, validateStep)\n",
    "\n",
    "print(\"7 5 9 2 Filtering\")\n",
    "train(load_student_model(), filtered_7_5_9_2_train_dataloader, validation_dataloader,\n",
    "      trainStep, validateStep)\n",
    "\n",
    "\n",
    "print(\"With studentTrainStepDistillationMNIST\")\n",
    "\n",
    "print(\"Normal\")\n",
    "train(load_student_model(), train_dataloader, validation_dataloader,\n",
    "      studentTrainStepDistillationMNIST, validateStep)\n",
    "\n",
    "print(\"7 Filtering\")\n",
    "train(load_student_model(), filtered_7_train_dataloader, validation_dataloader,\n",
    "      studentTrainStepDistillationMNIST, validateStep)\n",
    "\n",
    "print(\"7 5 Filtering\")\n",
    "train(load_student_model(), filtered_7_5_train_dataloader, validation_dataloader,\n",
    "      studentTrainStepDistillationMNIST, validateStep)\n",
    "\n",
    "print(\"7 5 9 Filtering\")\n",
    "train(load_student_model(), filtered_7_5_9_train_dataloader, validation_dataloader,\n",
    "      studentTrainStepDistillationMNIST, validateStep)\n",
    "\n",
    "print(\"7 5 9 2 Filtering\")\n",
    "train(load_student_model(), filtered_7_5_9_2_train_dataloader, validation_dataloader,\n",
    "      studentTrainStepDistillationMNIST, validateStep)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"global_step =  0\n",
    "n_train = len(filtered_train_data)\n",
    "print(n_train)\n",
    "\n",
    "T = 1.0 # temperature for distillation loss\n",
    "# Using a higher value for T produces a softer probability distribution over classes\n",
    "alpha = 1.0\n",
    "# trade-off between soft-target (st) cross-entropy and true-target (tt) cross-entropy;\n",
    "# loss = alpha * st + (1 - alpha) * tt\n",
    "\n",
    "print(\"Starting Training Loop...\")\n",
    "\n",
    "\n",
    "source_dice = []\n",
    "intervalle = []\n",
    "\n",
    "L_seg_list = []\n",
    "\n",
    "loss_list = []\n",
    "acc_list = []\n",
    "\n",
    "L_s_list = []\n",
    "\n",
    "compteur_plot = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    cpt_it = 0\n",
    "    with tqdm(total=n_train, desc=f'Epoch {epoch + 1}/{num_epochs}', unit='img') as pbar:\n",
    "\n",
    "        for i, data in enumerate(filtered_train_dataloader, 0):\n",
    "            student_model.train()\n",
    "            teacher_model.eval()\n",
    "\n",
    "            cpt_it += 1\n",
    "\n",
    "            X, y = data\n",
    "\n",
    "            X = X.to(\n",
    "                device=device, dtype=torch.float32)\n",
    "            y = y.to(\n",
    "                device=device, dtype=torch.long)\n",
    "\n",
    "            # Pass Data Trought net before optimizing everything\n",
    "\n",
    "            loss, acc = studentTrainStep(teacher_model, student_model, studentLoss, optimizer_global, X, y, T, alpha)\n",
    "            \n",
    "\n",
    "            L_global = loss\n",
    "\n",
    "            ###########################################################\n",
    "            # Evaluation on the Training Set\n",
    "            ###########################################################\n",
    "\n",
    "            student_model.eval()\n",
    "\n",
    "            intervalle.append(compteur_plot)\n",
    "            compteur_plot += 1\n",
    "\n",
    "            global_step += 1\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc\n",
    "\n",
    "            pbar.update(X.shape[0])\n",
    "            pbar.set_postfix(\n",
    "                **{'loss (batch)': epoch_loss/cpt_it, \"accuracy (batch)\": epoch_acc/cpt_it})\n",
    "            # pbar.set_postfix(**{'dice target': dice_target})\n",
    "\n",
    "            loss_list.append(loss.item())\n",
    "            acc_list.append(acc)\n",
    "# print(\"whole epoch target dice mean :\", sum(dice_score_target)/cpt_it)\n",
    "        if (epoch % batch_save_interval == 0):  # and epoch != 0 :\n",
    "\n",
    "            loss_smooth = smooth(loss_list, 0.99)\n",
    "            acc_smooth = smooth(acc_list, 0.99)\n",
    "\n",
    "            torch.save(student_model.state_dict(), saving_folder +\n",
    "                       \"/newtork_weigths/net_epoch{:}_acc{:.3f}_loss{:.3f}.pth\".format(epoch, epoch_acc/cpt_it, epoch_loss/cpt_it))\n",
    "\n",
    "            plt.figure(1)\n",
    "            plt.clf()\n",
    "            plt.plot(intervalle, acc_smooth, 'r-', label='Global accuracy')\n",
    "            plt.xlabel(\"iterations\")\n",
    "            plt.ylabel(\"Accuracy Value\")\n",
    "            plt.title(\"Accuracy Monitoring among training\")\n",
    "            plt.legend()\n",
    "            plt.savefig(saving_folder+\"/training_monitoring/acc_\" +\n",
    "                        str(epoch)+\"_epoch.png\")\n",
    "            plt.show()\n",
    "\n",
    "            plt.figure(1)\n",
    "            plt.clf()\n",
    "            plt.plot(intervalle, loss_smooth, 'r-', label='Global loss')\n",
    "            plt.xlabel(\"iterations\")\n",
    "            plt.ylabel(\"Loss Value\")\n",
    "            plt.title(\"Loss Monitoring among training\")\n",
    "            plt.legend()\n",
    "            plt.savefig(saving_folder+\"/training_monitoring/loss_\" +\n",
    "                        str(epoch)+\"_epoch.png\")\n",
    "            plt.show()\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fb5e5b18ee203a6716e147d61d87e90492f0bff6defd22583400cdaccb6acdb3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
